#!/usr/bin/env python3
"""Command-line interface for Transformer Roofline Analyzer."""

import argparse
import json
import os
import sys
from pathlib import Path

# Suppress transformers advisory warnings (e.g., missing PyTorch/TensorFlow)
# We only use config files, not model weights, so these frameworks aren't needed
os.environ.setdefault("TRANSFORMERS_NO_ADVISORY_WARNINGS", "1")

from transformer_roofline_analyzer.core import QueryConfig
from transformer_roofline_analyzer.parsers import Llama4ConfigParser, LlamaConfigParser


def load_config(model_name_or_path: str) -> dict:
    """
    Load model config from either HuggingFace model name or local file.

    Args:
        model_name_or_path: HuggingFace model name (e.g., "meta-llama/Llama-2-7b-hf")
                           or path to local config.json file

    Returns:
        dict: Model configuration dictionary

    Examples:
        >>> load_config("meta-llama/Llama-2-7b-hf")  # Downloads from HF
        >>> load_config("path/to/config.json")       # Loads local file
    """
    # Check if input is a local file path
    if os.path.exists(model_name_or_path):
        path = Path(model_name_or_path)
        if path.is_file():
            # Load from local JSON file
            with open(model_name_or_path) as f:
                return json.load(f)
        else:
            raise ValueError(f"Path exists but is not a file: {model_name_or_path}")

    # Otherwise, treat as HuggingFace model name
    try:
        from transformers import AutoConfig

        config = AutoConfig.from_pretrained(model_name_or_path)
        return config.to_dict()

    except ImportError:
        raise ImportError(
            "transformers library is required to download configs from HuggingFace.\n"
            "Install it with: pip install transformers"
        )
    except Exception as e:
        raise ValueError(
            f"Failed to load config from '{model_name_or_path}'.\n"
            f"  - If this is a HuggingFace model name, check that it exists and is accessible.\n"
            f"  - If this is a file path, check that the file exists.\n"
            f"Original error: {e}"
        )


PARSER_REGISTRY = {
    "llama": LlamaConfigParser,
    "llama4": Llama4ConfigParser,
    # Extend here for other model types
}


def compute_roofline_metrics(model_conf: dict, args: argparse.Namespace) -> None:
    model_type: str = model_conf.get("model_type", "").lower()

    parser_cls = PARSER_REGISTRY.get(model_type)
    if parser_cls is None:
        raise NotImplementedError(f"No parser for model_type: {model_type}")

    # Normalize config (e.g., set default torch_dtype) using parser-specific logic
    model_conf = parser_cls.normalize_config(model_conf)

    if len(args.cached_tokens) != len(args.input_tokens):
        raise ValueError(
            "`--cached-tokens` and `--input-tokens` must have the same number of elements."
        )

    if (args.batch_size is not None) and (args.batch_size % len(args.cached_tokens) != 0):
        raise ValueError(
            "`--batch-size` must be a multiple of the elements in `--cached-tokens` and `--input-tokens`."
        )

    cached_tokens: list[int] = args.cached_tokens * (
        int(args.batch_size / len(args.cached_tokens)) if args.batch_size is not None else 1
    )
    input_tokens: list[int] = args.input_tokens * (
        int(args.batch_size / len(args.input_tokens)) if args.batch_size is not None else 1
    )

    query_config: QueryConfig = QueryConfig(cached_tokens, input_tokens)

    parser = parser_cls(model_conf, query_config)
    parser.print_summary()


def main() -> None:
    parser = argparse.ArgumentParser(
        prog="transformer-roofline-analyzer",
        description="Analyze transformer model hardware requirements (compute and bandwidth).",
    )
    parser.add_argument(
        "model_name_or_config",
        help=(
            "HuggingFace model name (e.g., 'meta-llama/Llama-2-7b-hf') "
            "or path to local config.json file"
        ),
    )
    parser.add_argument(
        "--cached-tokens",
        type=int,
        nargs="+",
        default=[0],
        required=False,
        help="List of token counts already present in the KV-cache for each query in the batch (e.g., cached context tokens used during attention).",
    )
    parser.add_argument(
        "--input-tokens",
        type=int,
        nargs="+",
        default=[1],
        required=False,
        help="List of input token counts for each query in the batch (e.g., prompt or the tokens generated by the previous inference).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        required=False,
        help="The number of queries in a batch.",
    )

    args = parser.parse_args()

    # Load config using new utility function
    try:
        config = load_config(args.model_name_or_config)
    except Exception as e:
        print(f"Error loading config: {e}", file=sys.stderr)
        sys.exit(1)

    compute_roofline_metrics(config, args)


if __name__ == "__main__":
    main()
