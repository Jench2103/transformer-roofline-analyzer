# Transformer Roofline Calculator

**Transformer Roofline Calculator** is a CLI tool that estimates the compute (FLOPs) and memory bandwidth requirements of each layer‚Äîand the entire model‚Äîfor transformer architectures, using Hugging Face `config.json` files. It is particularly useful for analyzing hardware resource demands and performance trade-offs during model inference.

## ‚ú® Features

- Parses HuggingFace-compatible `config.json` with the following model types:
  - `llama`
  - `llama4`
- Reports:
  - Compute (FLOPs)
  - Bandwidth: weights, inputs, outputs
  - Operational intensity (FLOPs/Byte)
- Layer-wise and model-level summaries.
- Supports batching and KV-cache estimation.
- Useful for performance roofline modeling.

## üöÄ Getting Started

### Prerequisites

This project uses [Poetry](https://python-poetry.org/) for dependency management.

#### Requirements
- Python ‚â• 3.10
- Poetry ‚â• 2.0.0

#### Setup

```shell
# Clone the repo
git clone https://github.com/Jench2103/transformer-roofline-calculator.git
cd transformer-roofline-calculator

# Install dependencies using Poetry
poetry install

# Activate the virtual environment
eval $(poetry env activate)
```

### Usage

```shell
./transformer_roofline.py [OPTIONS] -- <config_path>
```

#### Example: Single Query

Analyze a single query with 1,048,576 cached tokens (in KV cache) and 1 input token:

```shell
./transformer_roofline.py --cached-tokens 1048576 --input-tokens 1 -- Llama-4-Scout-17B-16E-config.json
```

> **Note**: The `Llama-4-Scout-17B-16E-config.json` file must be provided by the user. You can download it from the corresponding model repository on [Hugging Face](https://huggingface.co/). The file should conform to the standard Hugging Face `config.json` format.

Sample output:

```
| Node                        |  Block Count  |       Compute |   Bandwidth (Weight) |   Bandwidth (Input) |   Bandwidth (Output) |   Operational Intensity |
|-----------------------------|---------------|---------------|----------------------|---------------------|----------------------|-------------------------|
| Attn - QKV_Proj             |    48 / 48    |  73.39 MFLOPs |            70.00 MiB |           10.00 KiB |            14.00 KiB |     999.76 mFLOPs/Bytes |
...
| Total (48 Blocks)           |      N/A      | 648.64 GFLOPs |            28.13 GiB |          192.01 GiB |             9.94 MiB |        2.74 FLOPs/Bytes |

Minimum Storage Requirement: (Weights) 28.13 GiB + (KV-cache) 192.00 GiB = 220.13 GiB
```

#### Example: Multiple Queries with Varying Tokens

Analyze two queries with different numbers of cached and input tokens:

```shell
./transformer_roofline.py --cached-tokens 1048576 1024 --input-tokens 1 1 -- Llama-4-Scout-17B-16E-config.json
```

#### Example: Batched Queries with Identical Token Counts

Analyze two queries with the same number of cached and input tokens:

```shell
./transformer_roofline.py --cached-tokens 1024 --input-tokens 1 --batch-size 2 -- Llama-4-Scout-17B-16E-config.json
```

#### Help Message

```
usage: transformer_roofline.py [-h] [--cached-tokens CACHED_TOKENS [CACHED_TOKENS ...]] [--input-tokens INPUT_TOKENS [INPUT_TOKENS ...]] [--batch-size BATCH_SIZE] config_path

positional arguments:
  config_path           Path to model config.json

options:
  -h, --help            show this help message and exit
  --cached-tokens CACHED_TOKENS [CACHED_TOKENS ...]
                        List of token counts already present in the KV-cache for each query in the batch (e.g., cached context tokens used during attention).
  --input-tokens INPUT_TOKENS [INPUT_TOKENS ...]
                        List of input token counts for each query in the batch (e.g., prompt or the tokens generated by the previous inference).
  --batch-size BATCH_SIZE
                        The number of queries in a batch.
```

#### Notes

- If `--batch-size` is not provided, it is inferred from the length of `--cached-tokens` and `--input-tokens`.
- Both `--cached-tokens` and `--input-tokens` support per-query customization by accepting space-separated lists.
- Both `--cached-tokens` and `--input-tokens` must have the same number of elements.
- If `--batch-size` is provided, it must be a multiple of the number of elements in `--cached-tokens` and `--input-tokens`.

## üìç Roadmap

- [x] Support model type `llama4` for models using earlier LLaMA architectures.
- [x] Support model type `llama` for models using earlier LLaMA architectures (e.g., LLaMA-2, LLaMA-3).
- [x] Support calculating the minimum storage requirement based on model configurations and given prompt-related parameters.

## ü§ù Contributing

Contributions, issues, and feature requests are welcome! Feel free to open a pull request or issue.

## üìù License

This project is licensed under the [MIT License](./LICENSE).
